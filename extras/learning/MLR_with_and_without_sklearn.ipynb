{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyKz5sh8yLod",
        "outputId": "e846609e-a7ab-4440-d1f2-2f5f9df1734e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'> (20640, 9)\n",
            "Data:\n",
            "X: (20640, 9) y: (20640,) X_train: (16512, 9) X_test: (4128, 9) y_train: (16512,) y_test: (4128,)\n",
            "#Linear Regression using Batch Gradient Descent using numpy#\n",
            "Thetas: [ 2.07185749  0.82894365  0.17853146 -0.13794939  0.15669182  0.01681517\n",
            " -0.04522857 -0.48705563 -0.45147126]\n",
            "Thetas Shape: (9,)\n",
            "MAE: 0.5476758462432642\n",
            "MSE: 0.5671852986082033\n",
            "#Linear Regression using Normal Equation Method using numpy#\n",
            "Thetas: [ 2.07194694  0.85438303  0.12254624 -0.29441013  0.33925949 -0.00230772\n",
            " -0.0408291  -0.89692888 -0.86984178]\n",
            "Thetas Shape: (9,)\n",
            "MAE: 0.5332001304956557\n",
            "MSE: 0.5558915986952441\n",
            "#Linear Regression using Normal Equation Method using sklearn\n",
            "X_train's shape: (16512, 8)\n",
            "coef shape (8,)\n",
            "Theta1...Thetan: [ 0.85438303  0.12254624 -0.29441013  0.33925949 -0.00230772 -0.0408291\n",
            " -0.89692888 -0.86984178]\n",
            "Intercept: 2.071946937378619\n",
            "MAE: 0.5332001304956566\n",
            "MSE: 0.5558915986952442\n",
            "#Linear Regression using Stochastic Gradient Descent using sklearn\n",
            "Theta1...Thetan: [ 0.87179658  0.12099137 -0.31555794  0.32073294 -0.0105368  -0.06844286\n",
            " -0.90230195 -0.86300283]\n",
            "Intercept: [2.05166858]\n",
            "MAE: 0.5315272144270915\n",
            "MSE: 0.5480901843309635\n",
            "#Linear Regression using Stochastic Gradient Descent and GridSearchCV using sklearn\n",
            "Best Hyper-Parameters: {'eta0': 0.001, 'max_iter': 5000}\n",
            "MAE: 0.53706727494225\n",
            "MSE: 0.555274484796614\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets, metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#from sklearn.datasets import fetch_california_housing\n",
        "from numpy.linalg import inv, pinv, LinAlgError\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression;\n",
        "from sklearn.linear_model import SGDRegressor;\n",
        "#from matplotlib import pyplot as plt\n",
        "#import seaborn as sns\n",
        "#from seaborn import heatmap;\n",
        "#from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "(Xtemp,y)=datasets.fetch_california_housing(return_X_y=True)\n",
        "#20640 instances, 8 numeric features + target is  the median house value for California districts,\n",
        "#expressed in hundreds of thousands of dollars, no missing data\n",
        "\n",
        "#adding a dummy variable\n",
        "X=np.ones((Xtemp.shape[0], Xtemp.shape[1]+1))\n",
        "print(type(X), X.shape)\n",
        "X[:,1:]=Xtemp;\n",
        "\n",
        "#train-test split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "#16512 in training set, 4128 in test set\n",
        "print(\"Data:\")\n",
        "print(\"X:\", X.shape, \"y:\", y.shape, \"X_train:\", X_train.shape, \"X_test:\", X_test.shape, \"y_train:\", y_train.shape, \"y_test:\", y_test.shape);\n",
        "\n",
        "#scale data\n",
        "ss=StandardScaler();\n",
        "ss.fit(X_train[:,1:])\n",
        "X_train[:,1:]=ss.transform(X_train[:,1:])\n",
        "X_test[:,1:]=ss.transform(X_test[:,1:])\n",
        "\n",
        "#Linear Regression using Batch Gradient Descent using numpy\n",
        "print(\"#Linear Regression using Batch Gradient Descent using numpy#\");\n",
        "niterations=1000;\n",
        "m=X_train.shape[0]\n",
        "n=X_train.shape[1]\n",
        "lr=0.01\n",
        "#theta=np.random.uniform(0,1,size=(n))\n",
        "theta=np.zeros(n)\n",
        "update=np.zeros(n)\n",
        "for i in range(niterations):\n",
        "    ypred=np.dot(X_train,theta);\n",
        "    error=ypred - y_train; #be mindful of this order in difference\n",
        "    for j in range(n):\n",
        "        update[j]=np.sum(error*((X_train.T)[j]))\n",
        "    theta=theta-(lr)*(1/m)*update\n",
        "print(\"Thetas:\", theta)\n",
        "print(\"Thetas Shape:\", theta.shape)\n",
        "pred=np.dot(X_test,theta);\n",
        "print(\"MAE:\", metrics.mean_absolute_error(y_test,pred))\n",
        "print(\"MSE:\", metrics.mean_squared_error(y_test,pred))\n",
        "\n",
        "\n",
        "#Linear Regression using Normal Equation Method using numpy\n",
        "print(\"#Linear Regression using Normal Equation Method using numpy#\")\n",
        "theta=np.zeros(X_train.shape[1])\n",
        "try:\n",
        "    XTXi=inv(np.dot(X_train.T,X_train))\n",
        "except LinAlgError:\n",
        "    XTXi=pinv(np.dot(X_train.T,X_train))\n",
        "XTy=np.dot(X_train.T,y_train)\n",
        "theta=np.dot(XTXi,XTy)\n",
        "print(\"Thetas:\", theta)\n",
        "print(\"Thetas Shape:\", theta.shape)\n",
        "predictions=np.dot(theta,X_test.T)\n",
        "print(\"MAE:\", metrics.mean_absolute_error(y_true=y_test,y_pred=predictions))\n",
        "print(\"MSE:\", metrics.mean_squared_error(y_true=y_test,y_pred=predictions))\n",
        "\n",
        "\n",
        "####Now using sklearn##########################################\n",
        "\n",
        "#Please note, it automatically adds dummy variable\n",
        "#so, removing dummy variable\n",
        "X_train=X_train[:,1:]\n",
        "X_test=X_test[:,1:]\n",
        "\n",
        "#Linear Regression using Normal Equation Method using sklearn\n",
        "print(\"#Linear Regression using Normal Equation Method using sklearn\")\n",
        "lr=LinearRegression();\n",
        "lr.fit(X_train,y_train);\n",
        "print(\"X_train's shape:\",X_train.shape)\n",
        "print(\"coef shape\",lr.coef_.shape)\n",
        "print(\"Theta1...Thetan:\", lr.coef_)\n",
        "print(\"Intercept:\",lr.intercept_)\n",
        "predictions=lr.predict(X_test);\n",
        "print(\"MAE:\", metrics.mean_absolute_error(y_true=y_test,y_pred=predictions))\n",
        "print(\"MSE:\", metrics.mean_squared_error(y_true=y_test,y_pred=predictions))\n",
        "\n",
        "#Linear Regression using Stochastic Gradient Descent using sklearn\n",
        "print(\"#Linear Regression using Stochastic Gradient Descent using sklearn\")\n",
        "sgd=SGDRegressor(eta0=0.01, max_iter=1000)\n",
        "sgd.fit(X_train,y_train)\n",
        "print(\"Theta1...Thetan:\", sgd.coef_)\n",
        "print(\"Intercept:\",sgd.intercept_)\n",
        "predictions=sgd.predict(X_test)\n",
        "print(\"MAE:\", metrics.mean_absolute_error(y_true=y_test,y_pred=predictions))\n",
        "print(\"MSE:\", metrics.mean_squared_error(y_true=y_test,y_pred=predictions))\n",
        "\n",
        "\n",
        "#Linear Regression using Stochastic Gradient Descent and GridSearchCV using sklearn\n",
        "print(\"#Linear Regression using Stochastic Gradient Descent and GridSearchCV using sklearn\")\n",
        "sgd1=SGDRegressor();\n",
        "param_grid={'eta0':[0.01, 0.001, 0.0001], 'max_iter':[5000,10000,20000, 30000] }\n",
        "gs=GridSearchCV(estimator=sgd1,param_grid=param_grid, cv=5)\n",
        "gs.fit(X_train, y_train);\n",
        "print(\"Best Hyper-Parameters:\", gs.best_params_)\n",
        "\n",
        "\n",
        "predictions=gs.predict(X_test);\n",
        "print(\"MAE:\", metrics.mean_absolute_error(y_true=y_test,y_pred=predictions))\n",
        "print(\"MSE:\", metrics.mean_squared_error(y_true=y_test,y_pred=predictions))"
      ]
    }
  ]
}